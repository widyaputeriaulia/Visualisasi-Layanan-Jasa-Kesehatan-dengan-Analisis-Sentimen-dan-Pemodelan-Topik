{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from utils import constant \n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def casefolding(str):\n",
    "    new_str = str.lower()\n",
    "    return new_str\n",
    "\n",
    "def stopword(str):\n",
    "    stop_words = set(stopwords.words('stopwordID.csv'))\n",
    "    word_tokens = word_tokenize(str)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "def remove_eng(str):\n",
    "    stop_words = set(stopwords.words('english.csv')) \n",
    "    word_tokens = word_tokenize(str) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]  \n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "#remove sentence which contains only one word\n",
    "def removeSentence(str): \n",
    "    word = str.split()\n",
    "    wordCount = len(word)\n",
    "    if(wordCount<=1):\n",
    "        str = ''\n",
    "    \n",
    "    return str\n",
    "\n",
    "\n",
    "\n",
    "def cleaning(str):\n",
    "    #remove punctuations\n",
    "    str = re.sub(r'[^\\w]|_',' ',str)\n",
    "    #remove digit from string\n",
    "    str = re.sub(\"\\S*\\d\\S*\", \"\", str).strip()\n",
    "    #removeHashtag\n",
    "    str = re.sub('#[^\\s]+','',str)\n",
    "    #remove non-ascii\n",
    "    str = unicodedata.normalize('NFKD', str).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    #to lowercase\n",
    "    str = str.lower()\n",
    "    #Remove additional white spaces\n",
    "    str = re.sub('[\\s]+', ' ', str)\n",
    "    \n",
    "    return str\n",
    "    \n",
    "#slang word\n",
    "def normalize_slang_word(str):\n",
    "    text_list = str.split(' ')\n",
    "    slang_words_raw = pd.read_csv('slang_word_list.csv', sep=',', header=None)\n",
    "    slang_word_dict = {}\n",
    "    \n",
    "    for item in slang_words_raw.values:\n",
    "        slang_word_dict[item[0]] = item[1]\n",
    "        \n",
    "        for index in range(len(text_list)):\n",
    "            if text_list[index] in slang_word_dict.keys():\n",
    "                text_list[index] = slang_word_dict[text_list[index]]\n",
    "    \n",
    "    return ' '.join(text_list) \n",
    "\n",
    "def normalize_bahasa_jawa(str):\n",
    "    text_list = str.split(' ')\n",
    "    jawa_raw = pd.read_csv('bahasa jawa.csv', sep=',', header=None)\n",
    "    jawa_dict = {}\n",
    "    \n",
    "    for item in jawa_raw.values:\n",
    "        jawa_dict[item[0]] = item[1]\n",
    "        \n",
    "        for index in range(len(text_list)):\n",
    "            if text_list[index] in jawa_dict.keys():\n",
    "                text_list[index] = jawa_dict[text_list[index]]\n",
    "    \n",
    "    return ' '.join(text_list) \n",
    "\n",
    "def remove_repeated_character(str):\n",
    "    str = re.sub(r'(.)\\1{2,}', r'\\1', str)\n",
    "    \n",
    "    return str\n",
    "\n",
    "def join_phrase(str):\n",
    "\n",
    "    text_list = str.split(' ')\n",
    "\n",
    "    negation_word = ['tidak','kurang','belum','cukup','sangat','bagian']\n",
    "    for index in range(len(text_list)):\n",
    "        for kt in negation_word:\n",
    "            if text_list[index] == kt:\n",
    "                if index < len(text_list) - 1:\n",
    "                    text_list[index] = text_list[index] + '_' + text_list[index + 1]\n",
    "                    text_list[index + 1] = ''\n",
    "                else:\n",
    "                    text_list[index] = ''\n",
    "    return (' '.join(text_list))\n",
    "\n",
    "def normalize_emoticon(str):\n",
    "        text_list = str.split(' ')\n",
    "        emoticon_list = constant.EMOTICON_LIST\n",
    "        for index in range(len(text_list)):\n",
    "            for key, value in emoticon_list:\n",
    "                if text_list[index] in value:\n",
    "                    text_list[index] = key\n",
    "        return ' '.join(text_list)\n",
    "\n",
    "def sentence_tokenization(s):\n",
    "    sentences_list = sent_tokenize(s)\n",
    "    \n",
    "    return sentences_list\n",
    "\n",
    "def split_sentences(str):\n",
    "    kata = [\"tapi\",\"Tapi\",\"Tetapi\",\"namun\",\"Namun\",\"tetapi\",\"saran\"]\n",
    "    word_tokens = word_tokenize(str) \n",
    "    sent = []\n",
    "    sentences = []\n",
    "    for w in word_tokens:\n",
    "        if w in kata:\n",
    "            sentences.append(' '.join(sent))\n",
    "            sent = []\n",
    "        else:\n",
    "            sent.append(w)\n",
    "    sentences.append(' '.join(sent))\n",
    "    return sentences\n",
    "\n",
    "def stemmingIndo(str):\n",
    "    factory = StemmerFactory()\n",
    "    #stemming bahasa indonesia\n",
    "    stemmer = factory.create_stemmer()\n",
    "    return stemmer.stem(str)\n",
    "\n",
    "\n",
    "def preprocessing(str):\n",
    "    str = normalize_slang_word(str)\n",
    "#     str = normalize_emoticon(str)\n",
    "#     str = cleaning(str) \n",
    "#     str = remove_repeated_character(str)\n",
    "#     str = normalize_bahasa_jawa(str)\n",
    "#     str = join_phrase(str)\n",
    "#     str = ''.join(str)\n",
    "#     str = str.replace('diterjemahkan',' ')\n",
    "#     str = str.replace('google',' ')\n",
    "#     str = str.replace('salah satu','')\n",
    "#     str = str.replace('terima kasih','terima_kasih')\n",
    "#     str = str.replace('rumah sakit','rumah_sakit')\n",
    "#     str = str.replace('rawat inap','rawat_inap')\n",
    "#     str = str.replace('panti rapih','panti_rapih')\n",
    "    str = stopword(str)\n",
    "#     str = remove_eng(str)\n",
    "#     str = removeSentence(str)\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS rekomended letknya wa Bethesda adalah rs swasta', 'cukup_ekonomis dibanding rs swasta lainya oh ya pengalaman aku biar bpjs rs tetap memberikan mutu yg baik', 'obat misal akan diberikan paten nah bagi yg anter berobat rs jangan kawatir susah cari makanan rs cafe jasmine toko roti jual jajan dlm rs atm bagi yg bth duit cash kalau luar rs atm bri rs jugaa dilengkapi kantin pintu barat masuk pengunjung nah yg lebih famous bakso nih sampingny bakso bethesda bakso enak harga lumayan premium seporsi rb kalau favorit ku bakso pak topi jual luar rs bethesda tepat seberang novotel hihihi']\n"
     ]
    }
   ],
   "source": [
    "kata =[\"RS rekomended letknya wa Bethesda adalah rs swasta tapi cukup_ekonomis dibanding rs swasta lainya oh ya pengalaman aku biar bpjs rs tetap memberikan mutu yg baik Tapi obat misal akan diberikan paten nah bagi yg anter berobat rs jangan kawatir susah cari makanan rs cafe jasmine toko roti jual jajan dlm rs atm bagi yg bth duit cash kalau luar rs atm bri rs jugaa dilengkapi kantin pintu barat masuk pengunjung nah yg lebih famous bakso nih sampingny bakso bethesda bakso enak harga lumayan premium seporsi rb kalau favorit ku bakso pak topi jual luar rs bethesda tepat seberang novotel hihihi\"]\n",
    "kt = ' '.join(kata)\n",
    "benar = split_sentences(kt)\n",
    "print(benar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = pd.read_excel('E:\\\\kuliah\\\\Pengolahan Bahasa Alami\\\\Bismillah Skripsi Baru\\\\Tugas Akhir\\\\File Positif v4.xlsx') #read excel file\n",
    "#fo = pd.read_csv('review JIH.csv',error_bad_lines=False)\n",
    "review = fo['Review']\n",
    "workbook = xlsxwriter.Workbook('E:\\\\kuliah\\\\Pengolahan Bahasa Alami\\\\Bismillah Skripsi Baru\\\\Tugas Akhir\\\\File Positif v4.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "#type(review)\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "rowHeaders = ['Review']\n",
    "worksheet.write_row(row, col,  tuple(rowHeaders))\n",
    "        \n",
    "for p in review:\n",
    "    f = str(p)\n",
    "#     st = sentence_tokenization(f)\n",
    "    sf = preprocessing(f)\n",
    "    rowValues = [sf]\n",
    "    row += 1\n",
    "    worksheet.write_row(row, col, tuple(rowValues))\n",
    "   \n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = pd.read_excel('E:\\\\kuliah\\\\Pengolahan Bahasa Alami\\\\Bismillah Skripsi Baru\\\\file skripsi\\\\kumpul.xlsx') #read excel file\n",
    "#fo = pd.read_csv('review JIH.csv',error_bad_lines=False)\n",
    "review = fo['Review']\n",
    "\n",
    "#print(review)\n",
    "workbook = xlsxwriter.Workbook('file skripsi\\kumpul.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "#type(review)\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "rowHeaders = ['Review']\n",
    "worksheet.write_row(row, col,  tuple(rowHeaders))\n",
    "        \n",
    "for p in review:\n",
    "    f = str(p)\n",
    "    reviewrs = preprocessing(f)\n",
    "    rowValues = [reviewrs]\n",
    "    row += 1\n",
    "    worksheet.write_row(row, col, tuple(rowValues))\n",
    "   \n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo = pd.read_excel('E:\\\\kuliah\\Pengolahan Bahasa Alami\\\\Bismillah Skripsi Baru\\\\Data_Bersih\\\\Positif sw 2.xlsx') #read excel file\n",
    "#fo = pd.read_csv('review JIH.csv',error_bad_lines=False)\n",
    "review = fo['Review']\n",
    "workbook = xlsxwriter.Workbook('E:\\\\kuliah\\\\Pengolahan Bahasa Alami\\\\Bismillah Skripsi Baru\\\\Data_Bersih\\\\Positif sw 2.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "#type(review)\n",
    "row = 0\n",
    "col = 0\n",
    "\n",
    "rowHeaders = ['Review']\n",
    "worksheet.write_row(row, col,  tuple(rowHeaders))\n",
    "        \n",
    "for p in review:\n",
    "    f = str(p)\n",
    "    st = preprocessing(f)\n",
    "    rowValues = [st]\n",
    "    row += 1\n",
    "    worksheet.write_row(row, col, tuple(rowValues))\n",
    "   \n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
